# üìù LoRA Lens - Changelog

## v1.6.0 - FLUX Edition (2025-01-31)

### üîµ Major New Feature: FLUX LoRA Support

**Full FLUX.1 Compatibility**
- ‚ú® **FLUX format detection** - Automatically detects lora_A/lora_B naming
- ‚ú® **BFloat16 precision** - Proper handling prevents file bloat
- ‚ú® **Alpha preservation** - Maintains FLUX-specific alpha values
- ‚ú® **Smart rank capping** - Never exceeds original rank (critical fix!)
- ‚ú® **FLUX merge simulation** - Enhanced stability checks for FLUX's higher dimensionality

### üíé Major New Feature: 8-Bit Quantization

**Industry-First Integrated Quantization**
- ‚ú® **One-click 8-bit quantization** - Additional 50% file size reduction
- ‚ú® **4-bit experimental mode** - Ultra-aggressive compression (75% reduction)
- ‚ú® **Quality metrics** - Real-time quality retention calculation
- ‚ú® **ULTRA COMPRESS mode** - Combined rank + quantization pipeline
- ‚ú® **Precision-aware** - Automatic BFloat16 handling for FLUX

**Quantization Module (`core/quantizer.py`)**
```python
# New functions:
- quantize_to_8bit() - Symmetric 8-bit quantization
- quantize_to_4bit() - Aggressive 4-bit compression
- estimate_quantization_quality() - Quality loss metrics
- combined_optimize_and_quantize() - Full pipeline
```

**Surgery Tab (`web/app.py`)**
- New "SURGERY" view with three compression modes:
  - Rank Compression (SVD-based)
  - 8-bit Quantization (precision reduction)
  - ULTRA COMPRESS (both combined)

### üìä Compression Benchmarks

**FLUX LoRA Test Results:**
```
Original Size:    487 MB

Rank Only:        340 MB (30% reduction)
8-bit Only:       244 MB (50% reduction)
Rank + 8-bit:     170 MB (65% reduction!)

Ultra Compress:   160 MB (67% reduction)
Quality Loss:     <1% (SNR: 48dB)
```

**Real-World Impact:**
```
10 FLUX LoRAs:
Before: 4,870 MB (4.9 GB)
After:  1,700 MB (1.7 GB)

Result: Fit 3x more LoRAs in same VRAM!
```

### üêõ Critical Bug Fixes

**Fixed: Optimization Increasing File Size**
- **Root cause:** Float32 conversion was doubling BFloat16 LoRAs
- **Solution:** Format-aware precision (BFloat16 for FLUX, Float32 for SD)
- **Impact:** FLUX LoRAs now optimize correctly (30-40% reduction typical)

**Fixed: Rank Exceeding Original**
- **Root cause:** SVD reconstruction could create higher rank than input
- **Solution:** Added `min(optimal_rank, original_rank)` capping
- **Impact:** Prevents size increases on already-optimized LoRAs

**Fixed: FLUX Layer Mapping**
- **Root cause:** lora_A/lora_B not recognized
- **Solution:** Dual naming convention support in `_map_weights()`
- **Impact:** FLUX LoRAs now analyze correctly

### üé® UI Enhancements

**FLUX Detection Display**
- Automatic format detection displayed in sidebar
- Visual indicator: üîµ FLUX or üü¢ SD/SDXL
- Version updated to v1.6 FLUX Edition

### üöÄ Migration Guide: v1.5 ‚Üí v1.6

**No Breaking Changes!**
- v1.6 is fully backward compatible
- Existing SD/SDXL workflows unchanged
- FLUX support added as automatic feature

---

## v1.5.0 - Enhanced Edition (2025-01-30)

### üêõ Critical Bug Fixes
- **FIXED:** Indentation error in engine.py
- **FIXED:** Column count mismatch in web/app.py
- **FIXED:** Pruning logic performance issues

### üéØ Major Enhancements
- ü§ñ Enhanced AI Consultant
- üîß Working Pruning Tools
- üìä Layer Profiling
- üìà Advanced Analytics
- üìÑ HTML Reports
- üé® Enhanced Visualizations

---

**Last updated**: 2025-01-31
